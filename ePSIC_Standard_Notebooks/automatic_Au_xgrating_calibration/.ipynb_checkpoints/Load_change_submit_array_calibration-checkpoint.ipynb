{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58dd9a0-58ac-4bca-8fd0-483b18bdfa7f",
   "metadata": {},
   "source": [
    "In this notebook one can:\n",
    "- load a notebook's settings as a dictionary\n",
    "- change it \n",
    "- save it as a new notebook \n",
    "- submit it as an array job to SLURM cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd346c1-bdbf-4f9b-84ca-9021e632d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/dls_sw/e02/software/epsic_tools')\n",
    "import epsic_tools.api as ep\n",
    "import pprint\n",
    "import re\n",
    "import subprocess\n",
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import time\n",
    "\n",
    "year = '2024'\n",
    "session = 'mgXXXXX-X'\n",
    "au_cal_folder = 'Au_xgrating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06978039-4b67-4ae2-a1b7-be2a9fa173ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = time.strftime(\"%s%s%s_%s%s%s\"%(time.gmtime()[0], \n",
    "                               time.gmtime()[1], \n",
    "                               time.gmtime()[2],\n",
    "                              time.gmtime()[3],\n",
    "                              time.gmtime()[4],\n",
    "                              time.gmtime()[5]))\n",
    "print(current)\n",
    "starting_notebook_path = os.getcwd() #'/dls/science/groups/e02/Sample_data/Test_data_ePSIC_User_notebooks/scripts_folder'\n",
    "starting_notebook_name = 'au_xgrating_cal_submit' #'template_BraggAnalysis-submit'\n",
    "nb = ep.notebook_utils.NotebookHelper(starting_notebook_path, starting_notebook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eba8ff-6918-41fa-8159-0d6d4d7fa380",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_settings = nb.get_settings(1) # settings should be cell index 1\n",
    "old_settings = old_settings.split(' ')\n",
    "old_keys = [i.split('=')[0] for i in old_settings]\n",
    "old_vals = [i.split('=')[1] for i in old_settings]\n",
    "old_dict = dict(zip(old_keys, old_vals))\n",
    "pprint.pprint(old_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6ccd4-64d0-42c1-b48c-c51f4180cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the root directory for the Merlin folders\n",
    "merlin_root = '/dls/e02/data/' + year + '/' + session + '/processing/Merlin/' + au_cal_folder\n",
    "print(merlin_root)\n",
    "hdf5_file_paths = glob.glob(merlin_root+ '/*/*.hdf5', recursive=True)\n",
    "\n",
    "# Output the paths\n",
    "hdf5_file_paths.sort()\n",
    "print(len(hdf5_file_paths))\n",
    "print(*hdf5_file_paths, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f19541-a7e5-4e0a-8f03-d26206a53467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some changes in new setting\n",
    "# log files from the cluster jobs and the bash script will be saved here:\n",
    "code_path = merlin_root + '/cluster_logs'\n",
    "if not os.path.exists(code_path):\n",
    "    os.mkdir(code_path)\n",
    "\n",
    "concurrent_jobs = 3 #Integer number of concurrent jobs to run in the array\n",
    "\n",
    "new_notebook_paths_list = []\n",
    "for file in hdf5_file_paths:\n",
    "    # update the settings\n",
    "    new_setting = old_dict.copy()\n",
    "    new_setting['file_path'] = file\n",
    "    new_setting['save_path_name'] = 'automatic_Au_calibration'\n",
    "    pprint.pprint(new_setting)\n",
    "\n",
    "    save_path = os.path.join(os.path.dirname(file), new_setting['save_path_name'])\n",
    "    print(save_path)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    new_notebook_path = os.path.join(save_path, 'submitted_notebook.ipynb')\n",
    "    nb.set_settings(new_setting, new_notebook_path)\n",
    "    print(f'new notebook path: {new_notebook_path}')\n",
    "    new_notebook_paths_list.append(new_notebook_path)\n",
    "\n",
    "note_book_path_file = os.path.join(code_path, 'notebook_list.txt')\n",
    "with open (note_book_path_file, 'w') as f:\n",
    "    f.write(\n",
    "        '\\n'.join(new_notebook_paths_list)\n",
    "    )\n",
    "\n",
    "bash_script_path = os.path.join(code_path, 'cluster_submit.sh')\n",
    "with open (bash_script_path, 'w') as f:\n",
    "    f.write('''#!/usr/bin/env bash\n",
    "#SBATCH --partition cs04r\n",
    "#SBATCH --job-name epsic_notebook\n",
    "#SBATCH --time 05:00:00\n",
    "#SBATCH --nodes 1\n",
    "#SBATCH --tasks-per-node 1\n",
    "#SBATCH --mem 200G\n",
    "'''\n",
    "f\"#SBATCH --array=0-{len(new_notebook_paths_list)-1}%{int(concurrent_jobs)}\\n\"\n",
    "f\"#SBATCH --error={code_path}{os.sep}logs_{current}{os.sep}error_%j.out\\n\"\n",
    "f\"#SBATCH --output={code_path}{os.sep}logs_{current}{os.sep}output_%j.out\\n\"\n",
    "f\"module load python/epsic3.10\\n\"\n",
    "f\"mapfile -t paths_array < {note_book_path_file}\\n\"\n",
    "'''\n",
    "echo ${paths_array[$SLURM_ARRAY_TASK_ID]}\n",
    "jupyter nbconvert --to notebook --inplace --ClearMetadataPreprocessor.enabled=True ${paths_array[$SLURM_ARRAY_TASK_ID]}\n",
    "jupyter nbconvert --to notebook --allow-errors --execute ${paths_array[$SLURM_ARRAY_TASK_ID]}\n",
    "\n",
    "'''\n",
    "           )\n",
    "        \n",
    "sshProcess = subprocess.Popen(['ssh',\n",
    "                               '-tt',\n",
    "                               'wilson'],\n",
    "                               stdin=subprocess.PIPE, \n",
    "                               stdout = subprocess.PIPE,\n",
    "                               universal_newlines=True,\n",
    "                               bufsize=0)\n",
    "sshProcess.stdin.write(\"ls .\\n\")\n",
    "sshProcess.stdin.write(\"echo END\\n\")\n",
    "sshProcess.stdin.write(f\"sbatch {bash_script_path}\\n\")\n",
    "sshProcess.stdin.write(\"uptime\\n\")\n",
    "sshProcess.stdin.write(\"logout\\n\")\n",
    "sshProcess.stdin.close()\n",
    "\n",
    "\n",
    "for line in sshProcess.stdout:\n",
    "    if line == \"END\\n\":\n",
    "        break\n",
    "    print(line,end=\"\")\n",
    "\n",
    "#to catch the lines up to logout\n",
    "for line in  sshProcess.stdout: \n",
    "    print(line,end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - EPSIC [DLS Conda]",
   "language": "python",
   "name": "conda-env-DLS_Conda-epsic3.10-kernel.json"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
