{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58dd9a0-58ac-4bca-8fd0-483b18bdfa7f",
   "metadata": {},
   "source": [
    "In this notebook one can:\n",
    "- load a notebook's settings as a dictionary\n",
    "- change it \n",
    "- save it as a new notebook \n",
    "- submit it as a job to SLURM cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd346c1-bdbf-4f9b-84ca-9021e632d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/dls_sw/e02/software/epsic_tools')\n",
    "import epsic_tools.api as ep\n",
    "import pprint\n",
    "import re\n",
    "import subprocess\n",
    "import os\n",
    "from __future__ import print_function,unicode_literals\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06978039-4b67-4ae2-a1b7-be2a9fa173ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_notebook_path = '/dls/e02/data/2024/mg37302-1/processing/Notebooks/'\n",
    "starting_notebook_name = 'template_BraggAnalysis-submit'\n",
    "nb = ep.notebook_utils.NotebookHelper(starting_notebook_path, starting_notebook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eba8ff-6918-41fa-8159-0d6d4d7fa380",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_settings = nb.get_settings(1) # settings should be cell index 1\n",
    "old_settings = old_settings.split(' ')\n",
    "old_keys = [i.split('=')[0] for i in old_settings]\n",
    "old_vals = [i.split('=')[1] for i in old_settings]\n",
    "old_dict = dict(zip(old_keys, old_vals))\n",
    "pprint.pprint(old_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6ccd4-64d0-42c1-b48c-c51f4180cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hdf5_files(root_dir):\n",
    "    hdf5_files = []\n",
    "    # Loop through each subdirectory in the root directory\n",
    "    for subfolder in os.listdir(root_dir):\n",
    "        # Check if the subfolder matches the 'SPXX' pattern\n",
    "        if subfolder.startswith(\"Li_metal\"):\n",
    "            spxx_path = os.path.join(root_dir, subfolder)\n",
    "            # Loop through each dataset subfolder inside the SPXX directory\n",
    "            for dataset_subfolder in os.listdir(spxx_path):\n",
    "                dataset_path = os.path.join(spxx_path, dataset_subfolder)\n",
    "                # Check for .hdf5 files in the dataset subfolder\n",
    "                for file in os.listdir(dataset_path):\n",
    "                    if file.endswith('.hdf5'):\n",
    "                        # Append the full path of the .hdf5 file\n",
    "                        hdf5_files.append(os.path.join(dataset_path, file))\n",
    "    return hdf5_files\n",
    "\n",
    "# Specify the root directory for the Merlin folders\n",
    "merlin_root = '/dls/e02/data/2024/mg37302-1/processing/Merlin'\n",
    "# Get all .hdf5 files under the specified directory\n",
    "hdf5_file_paths = find_hdf5_files(merlin_root)\n",
    "\n",
    "# Output the paths\n",
    "hdf5_file_paths.sort()\n",
    "print(len(hdf5_file_paths))\n",
    "print(*hdf5_file_paths, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f19541-a7e5-4e0a-8f03-d26206a53467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some changes in new setting\n",
    "hours_between_submit = 2 #submit a new batch every X hours\n",
    "seconds_between_prints = hours_between_submit * 3600\n",
    "import time\n",
    "n_once = 5\n",
    "for i in range(0, len(hdf5_file_paths), n_once):\n",
    "    batch = hdf5_file_paths[i:i+n_once]\n",
    "    for path in batch:\n",
    "        file_path = path\n",
    "\n",
    "        print(file_path)\n",
    "        \n",
    "        new_setting = old_dict.copy()\n",
    "        new_setting['crop_q'] = ''\n",
    "        new_setting['raw_data_path'] = file_path\n",
    "        new_setting['save_path_name'] = 'cluster_processed'\n",
    "        \n",
    "        pprint.pprint(new_setting)\n",
    "        \n",
    "        os.path.dirname(new_setting['raw_data_path'])\n",
    "        save_path = os.path.join(os.path.dirname(new_setting['raw_data_path']), new_setting['save_path_name'])\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        # new_setting['save_path_name']\n",
    "        \n",
    "        # Save a new version of the notebook with new settings:\n",
    "        new_notebook_path = os.path.join(save_path, 'submitted_notebook.ipynb')\n",
    "        nb.set_settings(new_setting, new_notebook_path)\n",
    "        \n",
    "        print(f'new notebook path: {new_notebook_path}')\n",
    "        print(f\"#SBATCH --output={save_path}{os.sep}output_%j.out\\n\")\n",
    "        # Create a bash script to submit to SLURM \n",
    "        bash_script_path = os.path.join(save_path, 'cluster_submit.sh')\n",
    "        with open (bash_script_path, 'w') as f:\n",
    "            f.write('''#!/usr/bin/env bash\n",
    "#SBATCH --partition cs05r\n",
    "#SBATCH --job-name epsic_notebook\n",
    "#SBATCH --time 05:00:00\n",
    "#SBATCH --nodes 1\n",
    "#SBATCH --gpus-per-node 0\n",
    "#SBATCH --tasks-per-node 1\n",
    "#SBATCH --mem 256G\n",
    "        '''\n",
    "        f\"#SBATCH --error={save_path}{os.sep}error_%j.out\\n\"\n",
    "        f\"#SBATCH --output={save_path}{os.sep}output_%j.out\\n\"\n",
    "        f\"module load python/epsic3.10\\n\"\n",
    "        f\"jupyter nbconvert --to notebook --inplace --ClearMetadataPreprocessor.enabled=True {new_notebook_path}\\n\"\n",
    "        f\"jupyter nbconvert --to notebook --allow-errors --execute {new_notebook_path}\\n\"\n",
    "                   )\n",
    "        \n",
    "        sshProcess = subprocess.Popen(['ssh',\n",
    "                                       '-tt',\n",
    "                                       'wilson'],\n",
    "                                       stdin=subprocess.PIPE, \n",
    "                                       stdout = subprocess.PIPE,\n",
    "                                       universal_newlines=True,\n",
    "                                       bufsize=0)\n",
    "        sshProcess.stdin.write(\"ls .\\n\")\n",
    "        sshProcess.stdin.write(\"echo END\\n\")\n",
    "        sshProcess.stdin.write(f\"sbatch {bash_script_path}\\n\")\n",
    "        sshProcess.stdin.write(\"uptime\\n\")\n",
    "        sshProcess.stdin.write(\"logout\\n\")\n",
    "        sshProcess.stdin.close()\n",
    "        \n",
    "        \n",
    "        for line in sshProcess.stdout:\n",
    "            if line == \"END\\n\":\n",
    "                break\n",
    "            print(line,end=\"\")\n",
    "        \n",
    "        #to catch the lines up to logout\n",
    "        for line in  sshProcess.stdout: \n",
    "            print(line,end=\"\")\n",
    "            \n",
    "    time.sleep(seconds_between_prints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - EPSIC [DLS Conda]",
   "language": "python",
   "name": "conda-env-DLS_Conda-epsic3.10-kernel.json"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
